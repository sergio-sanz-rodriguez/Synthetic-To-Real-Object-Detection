{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This code implements the training, validation, and testing pipelines for the Kaggle competition titled \"Synthetic to Real Object Detection Challenge.\"\n",
    "\n",
    "The proposed model is based on PyTorch's Region-based CNN (R-CNN), specifically the [Faster R-CNN](https://pytorch.org/vision/master/models/faster_rcnn.html) implementation. In this competition, the ``ResNet50_FPN_v2`` backbone has been used.\n",
    "\n",
    "A key aspect of the proposed method is an augmentation-based regularization technique to improve generalization. Strong data augmentation techniques, such as rotation, zooming out, occlusions, color jitter, and resolution scaling, are applied.\n",
    "\n",
    "The model relies on two R-CNNs. The first model makes predictions on all images, while the second is used only when the first fails to detect any regions of interest. In other words, the second model acts as a fallback, increasing the likelihood of correctly generating bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Import custom libraries\n",
    "from modules.obj_detection_utils import collate_fn, prune_predictions, display_and_save_predictions, visualize_transformed_data, set_seeds, RandomTextureOcclusion, RandomCircleOcclusion\n",
    "from modules.obj_detection import ObjectDetectionEngine\n",
    "from modules.schedulers import FixedLRSchedulerWrapper\n",
    "from modules.common import Common\n",
    "from modules.obj_dect_dataloaders import ProcessDatasetCheerios\n",
    "from modules.faster_rcnn import StandardFasterRCNN\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two data augmentation pipelines are used in this project:\n",
    "\n",
    "1. **First Augmentation Pipeline:** This pipeline applies several transformations, including occlusions, flips, and zooming out, among others. Notably, a bush texture, ``T_Bush_Falcon.png``, was created using the ``Falcon editing software`` to simulate occlusions with plants. This augmentation technique is implemented in the ``RandomTextureOcclusion`` class.\n",
    "\n",
    "2. **Second Augmentation Pipeline:** This pipeline is similar to the first one, but with a key difference in how occlusions are applied. Instead of using plant textures, occlusions are simulated by randomly adding circles of different colors. The ``RandomCircleOcclusion`` class is used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First augmentation pipeline\n",
    "def get_transform_1(train, mean_std_norm):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomChoice([T.Resize(size, interpolation=InterpolationMode.BILINEAR) for size in range(580, 1080, 250)]))\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "        transforms.append(T.ColorJitter(contrast=0.2, saturation=0.2))\n",
    "        transforms.append(T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.5)))\n",
    "        transforms.append(T.RandomPerspective(distortion_scale=0.1, p=0.5))\n",
    "        transforms.append(T.RandomZoomOut(fill={tv_tensors.Image: (0.5, 0.5, 0.5), \"others\": 0}, side_range=(1.0, 1.5), p=0.5))\n",
    "        transforms.append(T.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=(0.5, 0.5, 0.5)))\n",
    "        transforms.append(RandomTextureOcclusion(plant_path=[\"T_Bush_Falcon.png\"], transparency=1.0, p=0.5)) #Synthetic texture\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    if mean_std_norm:\n",
    "        transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# Second augmentation pipeline\n",
    "def get_transform_2(train, mean_std_norm):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomChoice([T.Resize(size, interpolation=InterpolationMode.BILINEAR) for size in range(580, 1080, 250)]))\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "        transforms.append(T.ColorJitter(contrast=0.2, saturation=0.2))\n",
    "        transforms.append(T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.5)))\n",
    "        transforms.append(T.RandomPerspective(distortion_scale=0.1, p=0.5))\n",
    "        transforms.append(RandomCircleOcclusion(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)))\n",
    "        transforms.append(T.RandomZoomOut(fill={tv_tensors.Image: (0.5, 0.5, 0.5), \"others\": 0}, side_range=(1.0, 1.5), p=0.5))\n",
    "        transforms.append(T.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=(0.5, 0.5, 0.5)))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    if mean_std_norm:\n",
    "        transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes (ROI + background)\n",
    "NUM_CLASSES = 2\n",
    "BATCHES = 2\n",
    "\n",
    "IMAGE_DIR = r\"Synthetic_to_Real_Object_Detection_Full/data/train/images\"\n",
    "image = cv2.imread(os.path.join(IMAGE_DIR, \"000000000.png\"))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Image dimensions\n",
    "img_height, img_width = image.shape[:2]\n",
    "img_size = (img_height, img_width)\n",
    "\n",
    "# Use ther dataset and defined transformations for model_1.pth\n",
    "train_dataset_1 = ProcessDatasetCheerios(\n",
    "    root=r\"Synthetic_to_Real_Object_Detection_Full/data/train\",\n",
    "    image_path=\"images\",\n",
    "    label_path=\"labels\",\n",
    "    transforms=get_transform_1(train=True, mean_std_norm=False),\n",
    "    num_classes=NUM_CLASSES-1) # Background to be removed\n",
    "\n",
    "val_dataset_1 = ProcessDatasetCheerios(\n",
    "    root=r\"Synthetic_to_Real_Object_Detection_Full/data/val\",\n",
    "    image_path=\"images\",\n",
    "    label_path=\"labels\",\n",
    "    transforms=get_transform_1(train=False, mean_std_norm=False),\n",
    "    num_classes=NUM_CLASSES-1) # Background to be removed\n",
    "\n",
    "# Define training and validation data loaders\n",
    "train_dataloader_1 = torch.utils.data.DataLoader(\n",
    "    train_dataset_1,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader_1 = torch.utils.data.DataLoader(\n",
    "    val_dataset_1,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Use ther dataset and defined transformations for model_2.pth\n",
    "train_dataset_2 = ProcessDatasetCheerios(\n",
    "    root=r\"Synthetic_to_Real_Object_Detection_Full/data/train\",\n",
    "    image_path=\"images\",\n",
    "    label_path=\"labels\",\n",
    "    transforms=get_transform_2(train=True, mean_std_norm=False),\n",
    "    num_classes=NUM_CLASSES-1) # Background to be removed\n",
    "\n",
    "val_dataset_2 = ProcessDatasetCheerios(\n",
    "    root=r\"Synthetic_to_Real_Object_Detection_Full/data/val\",\n",
    "    image_path=\"images\",\n",
    "    label_path=\"labels\",\n",
    "    transforms=get_transform_2(train=False, mean_std_norm=False),\n",
    "    num_classes=NUM_CLASSES-1) # Background to be removed\n",
    "\n",
    "# Define training and validation data loaders\n",
    "train_dataloader_2 = torch.utils.data.DataLoader(\n",
    "    train_dataset_2,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader_2 = torch.utils.data.DataLoader(\n",
    "    val_dataset_2,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Dataloaders: Original and Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformations\n",
    "dataloader_tr = torch.utils.data.DataLoader(\n",
    "    ProcessDatasetCheerios(\n",
    "        root=r\"Synthetic_to_Real_Object_Detection_Full/data/train\",\n",
    "        image_path=\"images\",\n",
    "        label_path=\"labels\",\n",
    "        transforms=get_transform_1(train=True, mean_std_norm=False),\n",
    "        num_classes=NUM_CLASSES-1), # Background to be removed\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "dataloader_ntr = torch.utils.data.DataLoader(\n",
    "    ProcessDatasetCheerios(\n",
    "        root=r\"Synthetic_to_Real_Object_Detection_Full/data/train\",\n",
    "        image_path=\"images\",\n",
    "        label_path=\"labels\",\n",
    "        transforms=get_transform_1(train=False, mean_std_norm=False),\n",
    "        num_classes=NUM_CLASSES-1), # Background to be removed\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for idx, ((img_tr, target_tr), (img_ntr, target_ntr)) in enumerate(zip(dataloader_tr, dataloader_ntr)):   \n",
    "    for i in range(0, BATCHES):\n",
    "        visualize_transformed_data(img_ntr[i], target_ntr[i], img_tr[i], target_tr[i])\n",
    "    if idx > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instatiate the model_1\n",
    "model_1 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Define model name\n",
    "model_name_1 = \"model_1.pth\"\n",
    "\n",
    "# Create the optimizer\n",
    "params = [p for p in model_1.parameters() if p.requires_grad]\n",
    "\n",
    "# Create AdamW optimizer\n",
    "LR = 1e-5\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model_1.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create scheduler\n",
    "EPOCHS = 30\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7),\n",
    "    fixed_lr=1e-7,\n",
    "    fixed_epoch=EPOCHS)\n",
    "\n",
    "# Instantiate the engine with the created model and the target device\n",
    "engine = ObjectDetectionEngine(\n",
    "    model=model_1,\n",
    "    log_verbose=True,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name_1,                    # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\"],           # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=True,            # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader_1,        # Train dataloader\n",
    "    test_dataloader=val_dataloader_1,           # Val dataloader\n",
    "    optimizer=optimizer,                        # Optimizer    \n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps: effective batch size = batch_size x accumulation steps\n",
    "    apply_validation=True                       # Enable validation step\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate the model\n",
    "model_2 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Define model name\n",
    "model_name_2 = \"model_2.pth\"\n",
    "\n",
    "# Create the optimizer\n",
    "params = [p for p in model_2.parameters() if p.requires_grad]\n",
    "\n",
    "# Create AdamW optimizer\n",
    "LR = 1e-5\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model_2.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create scheduler\n",
    "EPOCHS = 30\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7),\n",
    "    fixed_lr=1e-7,\n",
    "    fixed_epoch=EPOCHS)\n",
    "\n",
    "# Instantiate the engine with the created model and the target device\n",
    "engine_2 = ObjectDetectionEngine(\n",
    "    model=model_2,\n",
    "    log_verbose=True,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine_2.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name_2,                    # Name of the model\n",
    "    save_best_model=[\"last\", \"loss\"],           # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=True,            # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader_2,        # Train dataloader\n",
    "    test_dataloader=val_dataloader_2,           # Val dataloader\n",
    "    optimizer=optimizer,                        # Optimizer    \n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=2,                       # Accumulation steps: effective batch size = batch_size x accumulation steps\n",
    "    apply_validation=True                       # Enable validation step\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_model(model_name: str, new_name: str):\n",
    "    old_name = model_name[0]\n",
    "    os.rename(old_name, new_name)\n",
    "    print(f\"Renamed {old_name} to {new_name}\")\n",
    "    \n",
    "# Find the model file with \"model_1_loss_epoch\" prefix and rename it\n",
    "model_name = glob.glob(str(MODEL_DIR / \"model_1_loss_epoch*.pth\"))\n",
    "new_model_name = str(MODEL_DIR / \"model_1.pth\")\n",
    "rename_model(model_name, new_model_name)\n",
    "\n",
    "# Find the model file with \"model_2_loss_epoch\" prefix and rename it\n",
    "model_name = glob.glob(str(MODEL_DIR / \"model_2_loss_epoch*.pth\"))\n",
    "new_model_name = str(MODEL_DIR / \"model_2.pth\")\n",
    "rename_model(model_name, new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the trained model_1\n",
    "model_1 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Load the parameters of the best model\n",
    "model_1 = Common().load_model(model_1, \"outputs\", \"model.pth\")\n",
    "\n",
    "# Make predictions with model_1.pth\n",
    "preds_1 = ObjectDetectionEngine(\n",
    "    model=model_1,\n",
    "    device=device).predict(\n",
    "        dataloader=val_dataloader_1,\n",
    "        prune_predictions = True,\n",
    "        score_threshold = 0.9,\n",
    "        mask_threshold = 0.9,    \n",
    "        iou_threshold = 0.01\n",
    "        )\n",
    "\n",
    "# Configuration parameters for visualization\n",
    "BOX_COLOR = \"blue\"\n",
    "WIDTH = round(max(img_height, img_width)/175)\n",
    "FONT_TYPE = r\"C:\\Windows\\Fonts\\arial.ttf\"\n",
    "FONT_SIZE = 48\n",
    "PRINT_LABELS = True\n",
    "\n",
    "# Display predictions\n",
    "display_and_save_predictions(\n",
    "    preds=preds_1,\n",
    "    dataloader=val_dataset_1,\n",
    "    box_color=BOX_COLOR,\n",
    "    width=WIDTH,\n",
    "    font_type=FONT_TYPE,\n",
    "    font_size=FONT_SIZE,\n",
    "    print_classes=True,\n",
    "    print_scores=True,\n",
    "    label_to_class_dict={1: 'cheerios'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the trained model_2\n",
    "model_2 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# Load the parameters of the best model\n",
    "model_2 = Common().load_model(model_2, \"outputs\", \"model_2.pth\")\n",
    "\n",
    "# Make predictions with model_2.pth\n",
    "preds_2 = ObjectDetectionEngine(\n",
    "    model=model_2,\n",
    "    device=device).predict(\n",
    "        dataloader=val_dataloader_2,\n",
    "        prune_predictions = True,\n",
    "        score_threshold = 0.9,\n",
    "        mask_threshold = 0.9,    \n",
    "        iou_threshold = 0.01\n",
    "        )\n",
    "\n",
    "# Configuration parameters for visualization\n",
    "BOX_COLOR = \"red\"\n",
    "WIDTH = round(max(img_height, img_width)/175)\n",
    "FONT_TYPE = r\"C:\\Windows\\Fonts\\arial.ttf\"\n",
    "FONT_SIZE = 48\n",
    "PRINT_LABELS = True\n",
    "\n",
    "# Display predictions\n",
    "display_and_save_predictions(\n",
    "    preds=preds_2,\n",
    "    dataloader=val_dataset_2,\n",
    "    box_color=BOX_COLOR,\n",
    "    width=WIDTH,\n",
    "    font_type=FONT_TYPE,\n",
    "    font_size=FONT_SIZE,\n",
    "    print_classes=True,\n",
    "    print_scores=True,\n",
    "    label_to_class_dict={1: 'cheerios'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Real Data\n",
    "1. Open an ananconda terminal\n",
    "2. Activate the EDU environment\n",
    "3. Execute ``python predict_v3.py --model1 \"outputs/model_1.pth\" --model2 \"outputs/model_2.pth\"``\n",
    "4. Execute ``python convert_preds_to_csv_v2.py``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
